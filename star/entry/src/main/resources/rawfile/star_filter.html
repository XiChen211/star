<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, viewport-fit=cover">
    <title>Physics Star Filter Pro (Max Brightness Mode)</title>
    <style>
        body,
        html {
            margin: 0;
            padding: 0;
            width: 100%;
            height: 100%;
            background-color: #000;
            overflow: hidden;
        }

        #videoInput {
            display: none;
        }

        #bufferCanvas {
            display: none;
        }

        #glCanvas {
            display: block;
            width: 100%;
            height: 100%;
            object-fit: contain;
        }
    </style>
</head>

<body>
    <video id="videoInput" playsinline autoplay muted></video>
    <canvas id="glCanvas"></canvas>
    <canvas id="bufferCanvas"></canvas>

    <!-- 性能显示面板 -->
    <div id="perfPanel" style="
        position: fixed;
        top: 10px;
        left: 10px;
        background: rgba(0,0,0,0.75);
        color: #0f0;
        font-family: monospace;
        font-size: 12px;
        padding: 8px 12px;
        border-radius: 6px;
        z-index: 9999;
        pointer-events: none;
        line-height: 1.5;
        white-space: pre;
    ">性能统计加载中...</div>

    <script async src="https://docs.opencv.org/4.8.0/opencv.js" onload="onOpenCvReady()"
        type="text/javascript"></script>

    <script id="vs-video" type="x-shader/x-vertex">
    attribute vec2 a_position;
    attribute vec2 a_texCoord;
    varying vec2 v_texCoord;
    void main() {
        gl_Position = vec4(a_position, 0.0, 1.0);
        v_texCoord = a_texCoord;
    }
</script>
    <script id="fs-video" type="x-shader/x-fragment">
    precision mediump float;
    uniform sampler2D u_image;
    varying vec2 v_texCoord;
    void main() {
        gl_FragColor = texture2D(u_image, v_texCoord);
    }
</script>

    <script id="vs-star" type="x-shader/x-vertex">
    attribute vec2 a_position;
    attribute float a_gradient;
    uniform vec2 u_resolution;
    varying float v_gradient;
    void main() {
        vec2 zeroToOne = a_position / u_resolution;
        vec2 zeroToTwo = zeroToOne * 2.0;
        vec2 clipSpace = zeroToTwo - 1.0;
        gl_Position = vec4(clipSpace * vec2(1, -1), 0.0, 1.0);
        v_gradient = a_gradient;
    }
</script>
    <script id="fs-star" type="x-shader/x-fragment">
    precision mediump float;
    uniform vec4 u_color;
    uniform float u_opacity;
    varying float v_gradient;

    void main() {
        vec4 white = vec4(1.0, 1.0, 1.0, 1.0);
        vec4 baseColor = vec4(u_color.rgb, 1.0);
        vec4 outColor;

        if (v_gradient < 0.1) {
            float t = v_gradient / 0.1;
            outColor = mix(white, baseColor, t);
        } else {
            outColor = baseColor;
        }

        float alphaFade = 1.0 - smoothstep(0.0, 1.0, v_gradient);
        gl_FragColor = vec4(outColor.rgb, alphaFade * u_opacity);
    }
</script>

    <script id="vs-pack" type="x-shader/x-vertex">
    // [中文注释] 通用打包顶点着色器
    // 这是一个直通 Shader，不做任何顶点变换，仅传递纹理坐标。
    attribute vec2 a_position;
    attribute vec2 a_texCoord;
    varying vec2 v_texCoord;
    void main() {
        gl_Position = vec4(a_position, 0.0, 1.0);
        v_texCoord = a_texCoord;
    }
</script>

    <script id="fs-pack-luma" type="x-shader/x-fragment">
    precision mediump float;
    uniform sampler2D u_image;
    uniform float u_width; 
    uniform float u_height;
    varying vec2 v_texCoord;

    // [中文注释] 亮度打包 Shader
    // 作用：将水平方向上连续的 4 个像素的亮度值 (Luma)，打包进一个 RGBA 像素的 4 个通道中。
    // 结果：输出图像的宽度是原始宽度的 1/4，实现了无损的亮度数据传输（相比于读取单通道纹理，利用了 RGBA 全部带宽）。
    void main() {
        // 计算单个像素在纹理坐标中的跨度
        float onePixel = 1.0 / u_width;
        
        // 亮度转换权重 (Standard BT.601)
        const vec3 W = vec3(0.299, 0.587, 0.114);
        
        // 采样中心调整
        // 我们当前渲染的一个片元 (Fragment) 对应原图中的 4 个水平像素。
        // 为了精确采样这 4 个像素，我们需要根据中心点进行偏移。
        // 偏移量分别为：-1.5, -0.5, +0.5, +1.5 个像素单位。
        
        float dx = onePixel;
        vec2 uv_center = v_texCoord; 
        
        // 分别采样 4 个像素
        vec4 p0 = texture2D(u_image, uv_center + vec2(-1.5 * dx, 0.0));
        vec4 p1 = texture2D(u_image, uv_center + vec2(-0.5 * dx, 0.0));
        vec4 p2 = texture2D(u_image, uv_center + vec2(0.5 * dx, 0.0));
        vec4 p3 = texture2D(u_image, uv_center + vec2(1.5 * dx, 0.0));
        
        // 计算亮度并写入 RGBA 通道
        gl_FragColor = vec4(
            dot(p0.rgb, W), // R 通道存储第 1 个像素的亮度
            dot(p1.rgb, W), // G 通道存储第 2 个像素的亮度
            dot(p2.rgb, W), // B 通道存储第 3 个像素的亮度
            dot(p3.rgb, W)  // A 通道存储第 4 个像素的亮度
        );
    }
</script>

    <script id="fs-downscale-color" type="x-shader/x-fragment">
    precision mediump float;
    uniform sampler2D u_image;
    varying vec2 v_texCoord;

    // [中文注释] 色彩降采样 Shader
    // 这是一个简单的纹理采样器，利用 GPU 的线性插值 (Linear Filtering) 自动实现图像缩小。
    // 用于生成低分辨率的“色彩图”，供星芒染色使用。
    void main() {
        gl_FragColor = texture2D(u_image, v_texCoord);
    }
</script>

    <script type="text/javascript">
        let video = document.getElementById('videoInput');
        let canvas = document.getElementById('glCanvas');
        let bufferCanvas = document.getElementById('bufferCanvas');

        let gl = canvas.getContext('webgl', { alpha: false, preserveDrawingBuffer: true });
        let bufferCtx = bufferCanvas.getContext('2d', { willReadFrequently: true });

        let streaming = false;
        let stream = null;
        let animationId = null;
        let currentFacingMode = 'environment';
        let isStaticMode = false;
        let uploadedImage = new Image();

        let programVideo, programStar, programPackLuma, programDownscaleColor;
        let textureVideo;
        let bufferVideoRect, bufferStar;

        // [Optim] Atlas FBO members
        let atlasFbo = null;
        let atlasTexture = null;
        let atlasWidth = 0;
        let atlasHeight = 0;
        let colorScale = 0.25; // 1/4 resolution for color
        let textureNeedsUpdate = false;

        let params = {
            threshold: 240,
            dilationEnabled: 1,
            dilationIter: 1,
            dilationSize: 1,
            minAreaEnabled: 1,
            minArea: 25,
            maxAreaEnabled: 1,
            maxAreaRatio: 1000,
            shapeFilterEnabled: 1,
            circularity: 0.3, // Reverted to 0.3
            detectionScale: 0.5, // [Optim] Downscale detection
            starLengthMin: 40,
            starLengthMax: 120,
            starOpacity: 0.8,
            starPoints: 4,
            starAngle: 0,
            starSaturation: 1.0
        };

        let matPool = {
            src: null, gray: null, thresh: null, kernel: null, contours: null, hierarchy: null,
            smallGray: null, smallThresh: null // [Optim] Aux mats
        };

        let WIDTH = 1080;
        let HEIGHT = 1440;

        // --- WebGL Helpers ---
        function createShader(gl, type, source) {
            const shader = gl.createShader(type);
            gl.shaderSource(shader, source);
            gl.compileShader(shader);
            if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
                gl.deleteShader(shader);
                return null;
            }
            return shader;
        }

        function createProgram(gl, vsId, fsId) {
            const vs = createShader(gl, gl.VERTEX_SHADER, document.getElementById(vsId).text);
            const fs = createShader(gl, gl.FRAGMENT_SHADER, document.getElementById(fsId).text);
            const program = gl.createProgram();
            gl.attachShader(program, vs);
            gl.attachShader(program, fs);
            gl.linkProgram(program);
            return program;
        }

        function initWebGL() {
            programVideo = createProgram(gl, 'vs-video', 'fs-video');
            programPackLuma = createProgram(gl, 'vs-pack', 'fs-pack-luma');
            programDownscaleColor = createProgram(gl, 'vs-pack', 'fs-downscale-color');

            const bufferData = new Float32Array([
                -1, -1, 0, 1, // Bottom-Left: UV(0,1) for Image? No. 
                // Standard Image: Top-Left is 0,0. GL Texture: Bottom-Left is 0,0.
                // Our vs-video flips Y? No, vs-video passes texCoord.
                // bufferData Layout: X, Y, U, V
                1, -1, 1, 1,
                -1, 1, 0, 0,
                1, 1, 1, 0
                // This V=0 at Top (Y=1) means texture is drawn correctly if image is stored "normally" (Top-Down)
                // BUT video texture in browser is often Top-Down.
                // Pack Logic: We want to map UV 0..1 to Screen.
            ]);
            bufferVideoRect = gl.createBuffer();
            gl.bindBuffer(gl.ARRAY_BUFFER, bufferVideoRect);
            gl.bufferData(gl.ARRAY_BUFFER, bufferData, gl.STATIC_DRAW);

            programStar = createProgram(gl, 'vs-star', 'fs-star');
            bufferStar = gl.createBuffer();

            textureVideo = gl.createTexture();
            gl.bindTexture(gl.TEXTURE_2D, textureVideo);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
        }

        // [中文注释] 初始化图集 Framebuffer (FBO)
        // 这里的“图集”用于存储两种数据：
        // 1. Pack 后的亮度数据 (占据主要区域)
        // 2. 降采样后的色彩数据 (占据顶部一小块区域)
        function initAtlasFBO(w, h) {
            // w, h 是输入视频的分辨率 (如 1080x1440)

            // 亮度图宽度仅需 1/4 (因为 4 个像素压成 1 个 RGBA)
            let packedW = Math.ceil(w / 4);

            // 色彩图高度为原图高度的 0.25 (可配置)，用于存缩略图
            let colorH = Math.ceil(h * colorScale);

            // FBO 总高度 = 原始高度 (用于存 Luma) + 色彩图高度
            let totalH = h + colorH;

            // 缓存机制：如果尺寸没变，就直接复用
            if (atlasFbo && atlasWidth === packedW && atlasHeight === totalH) return;

            if (atlasFbo) gl.deleteFramebuffer(atlasFbo);
            if (atlasTexture) gl.deleteTexture(atlasTexture);

            atlasWidth = packedW;
            atlasHeight = totalH;

            atlasTexture = gl.createTexture();
            gl.bindTexture(gl.TEXTURE_2D, atlasTexture);
            // 分配显存
            gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, atlasWidth, atlasHeight, 0, gl.RGBA, gl.UNSIGNED_BYTE, null);

            // [重要] 这里必须使用 NEAREST 过滤！
            // 因为亮度数据是"打包"存储的，如果使用 LINEAR 插值，会导致像素混合，
            // 使得解包出来的亮度值错误 (R 通道混入了 G 通道的值)。
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);

            atlasFbo = gl.createFramebuffer();
            gl.bindFramebuffer(gl.FRAMEBUFFER, atlasFbo);
            gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, atlasTexture, 0);

            let status = gl.checkFramebufferStatus(gl.FRAMEBUFFER);
            if (status !== gl.FRAMEBUFFER_COMPLETE) {
                console.error("FBO incomplete", status);
            }
            gl.bindFramebuffer(gl.FRAMEBUFFER, null);
        }

        // --- Interface ---
        window.updateParam = function (key, value) {
            params[key] = parseFloat(value);
        };

        // UI Debug Helper - Disabled
        function logToUI(msg, isError = false) {
            // Disabled
        }

        // Global Error Handler
        window.onerror = function (msg, url, lineNo, columnNo, error) {
            // logToUI(`Global Error: ${msg} @ ${lineNo}:${columnNo}`, true);
            return false;
        };

        window.switchCamera = function () {
            isStaticMode = false;
            currentFacingMode = (currentFacingMode === 'user') ? 'environment' : 'user';
            stopCamera();
            startCamera();
        };

        window.toggleCamera = function (shouldEnable) {
            if (shouldEnable) {
                if (isStaticMode) {
                    isStaticMode = false;
                    stopCamera();
                    startCamera();
                } else {
                    if (!streaming) startCamera();
                }
            } else {
                if (!isStaticMode) stopCamera();
            }
        };

        window.setZoom = function (value) {
            if (!stream) return;
            let track = stream.getVideoTracks()[0];
            let caps = track.getCapabilities();
            if (caps.zoom) {
                track.applyConstraints({ advanced: [{ zoom: value }] })
                    .catch(e => console.error("setZoom failed", e));
            }
        };

        window.setExposure = function (value) {
            if (!stream) return;
            let track = stream.getVideoTracks()[0];
            let caps = track.getCapabilities();
            if (caps.exposureCompensation) {
                track.applyConstraints({ advanced: [{ exposureCompensation: value }] })
                    .catch(e => console.error("setExposure failed", e));
            }
        };

        window.loadStaticImage = function (base64Str) {
            let len = base64Str ? base64Str.length : 0;
            logToUI('loadStaticImage called, len: ' + len);
            if (window.etsProxy) window.etsProxy.postMessage(JSON.stringify({ type: 'log', msg: 'loadStaticImage called, length: ' + len }));

            stopCamera();

            // Hide perf panel in static mode
            let pp = document.getElementById('perfPanel');
            if (pp) pp.style.display = 'none';

            // Auto-fix prefix if missing
            if (base64Str && !base64Str.startsWith('data:image')) {
                logToUI('Warning: Missing data prefix, auto-fixing...');
                if (window.etsProxy) window.etsProxy.postMessage(JSON.stringify({ type: 'log', msg: 'Adding missing data prefix' }));
                base64Str = 'data:image/jpeg;base64,' + base64Str;
            }

            uploadedImage.onerror = function (e) {
                logToUI('Image load FAILED', true);
                if (window.etsProxy) window.etsProxy.postMessage(JSON.stringify({ type: 'error', msg: 'Image load failed' }));
            };

            uploadedImage.onload = function () {
                logToUI(`Image loaded successfully: ${uploadedImage.width}x${uploadedImage.height}`);
                if (window.etsProxy) window.etsProxy.postMessage(JSON.stringify({ type: 'log', msg: 'Image loaded: ' + uploadedImage.width + 'x' + uploadedImage.height }));
                isStaticMode = true;
                streaming = true;
                textureNeedsUpdate = true;

                WIDTH = uploadedImage.width;
                HEIGHT = uploadedImage.height;

                const MAX_SIDE = 1920;
                if (WIDTH > MAX_SIDE || HEIGHT > MAX_SIDE) {
                    let ratio = WIDTH / HEIGHT;
                    if (WIDTH > HEIGHT) {
                        WIDTH = MAX_SIDE;
                        HEIGHT = Math.floor(MAX_SIDE / ratio);
                    } else {
                        HEIGHT = MAX_SIDE;
                        WIDTH = Math.floor(MAX_SIDE * ratio);
                    }
                }

                canvas.width = WIDTH;
                canvas.height = HEIGHT;
                bufferCanvas.width = WIDTH;
                bufferCanvas.height = HEIGHT;

                gl.viewport(0, 0, WIDTH, HEIGHT);

                initMats();
                if (animationId) cancelAnimationFrame(animationId);
                requestAnimationFrame(processVideo);
            };
            uploadedImage.src = base64Str;
        };

        window.captureImage = function () {
            if (!streaming) return;
            renderScene();
            let dataURL = canvas.toDataURL('image/png', 1.0);
            if (window.etsProxy) {
                window.etsProxy.postMessage(JSON.stringify({ type: 'capture', data: dataURL }));
            }
        };

        // 测试111111

        function onOpenCvReady() {
            logToUI("OpenCV Ready callback");
            initWebGL();
            // If we are already in static mode (image loaded before cv ready), force texture update
            if (isStaticMode) {
                textureNeedsUpdate = true;
                initMats();
            }
            if (window.etsProxy) window.etsProxy.postMessage(JSON.stringify({ type: 'status', msg: 'ready' }));
        }

        function initMats() {
            if (typeof cv === 'undefined') return;

            // Helper for safe deletion
            const safeDelete = (key) => {
                let obj = matPool[key];
                if (obj) {
                    try {
                        if (obj.delete && !obj.isDeleted) {
                            obj.delete();
                        }
                    } catch (e) {
                        // Ignore already deleted errors
                    }
                    matPool[key] = null;
                }
            };

            try {
                // 1. Cleanup ALL existing mats
                safeDelete('contours');
                safeDelete('hierarchy');
                safeDelete('kernel');
                safeDelete('src');
                safeDelete('gray');
                safeDelete('smallGray');
                safeDelete('smallThresh');
                safeDelete('thresh');

                // 2. Clear any other keys just in case
                for (let key in matPool) {
                    if (matPool[key] && matPool[key].delete) {
                        safeDelete(key);
                    }
                }

                // 3. Re-allocate
                matPool.src = new cv.Mat(HEIGHT, WIDTH, cv.CV_8UC4);
                matPool.gray = new cv.Mat(HEIGHT, WIDTH, cv.CV_8UC1);

                // [Optim] Init downscaled mats
                let sW = Math.floor(WIDTH * params.detectionScale);
                let sH = Math.floor(HEIGHT * params.detectionScale);
                matPool.smallGray = new cv.Mat(sH, sW, cv.CV_8UC1);
                matPool.smallThresh = new cv.Mat(sH, sW, cv.CV_8UC1);

                matPool.thresh = new cv.Mat(HEIGHT, WIDTH, cv.CV_8UC1);
                matPool.contours = new cv.MatVector();
                matPool.hierarchy = new cv.Mat();

                logToUI(`initMats success ${WIDTH}x${HEIGHT}`);
            } catch (e) {
                logToUI("initMats error: " + e, true);
                console.error(e);
            }
        }

        function getAverageColor(x, y, radius, imgData, width, height) {
            let r = 0, g = 0, b = 0, count = 0;
            let startX = Math.max(0, Math.floor(x - radius));
            let endX = Math.min(width - 1, Math.ceil(x + radius));
            let startY = Math.max(0, Math.floor(y - radius));
            let endY = Math.min(height - 1, Math.ceil(y + radius));

            for (let py = startY; py <= endY; py++) {
                for (let px = startX; px <= endX; px++) {
                    let idx = (py * width + px) * 4;
                    r += imgData[idx];
                    g += imgData[idx + 1];
                    b += imgData[idx + 2];
                    count++;
                }
            }
            if (count === 0) return { r: 255, g: 255, b: 255 };
            return { r: Math.round(r / count), g: Math.round(g / count), b: Math.round(b / count) };
        }


        function adjustSaturation(r, g, b, saturation) {
            if (saturation === 1.0) return { r, g, b };
            const gray = 0.299 * r + 0.587 * g + 0.114 * b;
            return {
                r: Math.min(255, Math.max(0, gray + (r - gray) * saturation)),
                g: Math.min(255, Math.max(0, gray + (g - gray) * saturation)),
                b: Math.min(255, Math.max(0, gray + (b - gray) * saturation))
            };
        }

        function addStarVertices(vertexArray, cx, cy, size, points, baseAngle, widthScale) {
            let r = size / 2;
            let numStreaks = points / 2;
            let angleStep = 180 / numStreaks;

            for (let i = 0; i < numStreaks; i++) {
                let currentAngleDeg = baseAngle + (i * angleStep);

                let streakLen = r;
                let streakWidth = 2.0 * widthScale;

                if (points === 8 && (i % 2 !== 0)) {
                    streakLen = r * 0.7;
                    streakWidth = 1.5 * widthScale;
                }

                let rad = currentAngleDeg * Math.PI / 180.0;
                let cos = Math.cos(rad);
                let sin = Math.sin(rad);

                let x1 = cx + cos * streakLen;
                let y1 = cy + sin * streakLen;
                let x2 = cx + Math.cos(rad + Math.PI / 2) * streakWidth;
                let y2 = cy + Math.sin(rad + Math.PI / 2) * streakWidth;
                let x3 = cx - cos * streakLen;
                let y3 = cy - sin * streakLen;
                let x4 = cx + Math.cos(rad - Math.PI / 2) * streakWidth;
                let y4 = cy + Math.sin(rad - Math.PI / 2) * streakWidth;

                vertexArray.push(x2, y2, 0.0);
                vertexArray.push(x1, y1, 1.0);
                vertexArray.push(x4, y4, 0.0);

                vertexArray.push(x2, y2, 0.0);
                vertexArray.push(x3, y3, 1.0);
                vertexArray.push(x4, y4, 0.0);
            }
        }

        function startCamera() {
            isStaticMode = false;
            stopCamera();

            // Show perf panel in camera mode
            let pp = document.getElementById('perfPanel');
            if (pp) pp.style.display = 'block';

            let constraints = {
                audio: false,
                video: {
                    facingMode: currentFacingMode,
                    width: { ideal: 1920 },
                    height: { ideal: 1440 }
                }
            };

            navigator.mediaDevices.getUserMedia(constraints).then(function (s) {
                stream = s;
                video.srcObject = stream;
                video.play();
                video.onplaying = () => { streaming = true; };

                video.oncanplay = function () {
                    let vw = video.videoWidth;
                    let vh = video.videoHeight;
                    if (vw && vh) {
                        WIDTH = vw;
                        HEIGHT = vh;
                    } else {
                        WIDTH = 1080; HEIGHT = 1440;
                    }

                    canvas.width = WIDTH;
                    canvas.height = HEIGHT;
                    bufferCanvas.width = WIDTH;
                    bufferCanvas.height = HEIGHT;

                    gl.viewport(0, 0, WIDTH, HEIGHT);

                    streaming = true;
                    initMats();
                    if (animationId) cancelAnimationFrame(animationId);
                    requestAnimationFrame(processVideo);
                };
            }).catch(err => {
                if (window.etsProxy) window.etsProxy.postMessage(JSON.stringify({ type: 'error', msg: err.toString() }));
            });
        }

        function stopCamera() {
            streaming = false;
            if (animationId) { cancelAnimationFrame(animationId); animationId = null; }
            if (stream) { stream.getTracks().forEach(t => t.stop()); stream = null; }
            video.pause();
            video.srcObject = null;
            gl.clearColor(0, 0, 0, 1);
            gl.clear(gl.COLOR_BUFFER_BIT);
        }

        let lastTime = Date.now();
        let frameCount = 0;

        // === 性能计时器 ===
        let perfStats = {
            getImageData: 0,
            cvtColor: 0,
            resize: 0, // [Optim] New metric
            threshold: 0,
            dilate: 0,
            findContours: 0,
            contourLoop: 0,
            webglDraw: 0,
            total: 0,
            sampleCount: 0
        };

        function logPerformance() {
            if (perfStats.sampleCount === 0) return;
            let n = perfStats.sampleCount;
            let msg = `[性能统计 - ${n}帧平均]\n` +
                `  总耗时: ${(perfStats.total / n).toFixed(2)}ms\n` +
                `  ├─ getImageData: ${(perfStats.getImageData / n).toFixed(2)}ms\n` +
                `  ├─ cvtColor: ${(perfStats.cvtColor / n).toFixed(2)}ms\n` +
                `  ├─ resize: ${(perfStats.resize / n).toFixed(2)}ms\n` +
                `  ├─ threshold: ${(perfStats.threshold / n).toFixed(2)}ms\n` +
                `  ├─ dilate: ${(perfStats.dilate / n).toFixed(2)}ms\n` +
                `  ├─ findContours: ${(perfStats.findContours / n).toFixed(2)}ms\n` +
                `  ├─ contourLoop: ${(perfStats.contourLoop / n).toFixed(2)}ms\n` +
                `  └─ webglDraw: ${(perfStats.webglDraw / n).toFixed(2)}ms`;
            console.log(msg);

            // 更新页面上的性能面板
            let panel = document.getElementById('perfPanel');
            if (panel) {
                panel.textContent = `性能统计 (${n}帧平均)\n` +
                    `分辨率: ${WIDTH} x ${HEIGHT}\n` +
                    `总耗时: ${(perfStats.total / n).toFixed(1)}ms\n` +
                    `─────────────────\n` +
                    `getImageData: ${(perfStats.getImageData / n).toFixed(1)}ms\n` +
                    `cvtColor:     ${(perfStats.cvtColor / n).toFixed(1)}ms\n` +
                    `threshold:    ${(perfStats.threshold / n).toFixed(1)}ms\n` +
                    `dilate:       ${(perfStats.dilate / n).toFixed(1)}ms\n` +
                    `findContours: ${(perfStats.findContours / n).toFixed(1)}ms\n` +
                    `contourLoop:  ${(perfStats.contourLoop / n).toFixed(1)}ms\n` +
                    `webglDraw:    ${(perfStats.webglDraw / n).toFixed(1)}ms`;
            }

            if (window.etsProxy) {
                window.etsProxy.postMessage(JSON.stringify({
                    type: 'perf', data: {
                        total: (perfStats.total / n).toFixed(2),
                        getImageData: (perfStats.getImageData / n).toFixed(2),
                        cvtColor: (perfStats.cvtColor / n).toFixed(2),
                        threshold: (perfStats.threshold / n).toFixed(2),
                        dilate: (perfStats.dilate / n).toFixed(2),
                        findContours: (perfStats.findContours / n).toFixed(2),
                        contourLoop: (perfStats.contourLoop / n).toFixed(2),
                        webglDraw: (perfStats.webglDraw / n).toFixed(2)
                    }
                }));
            }
            // 重置
            for (let key in perfStats) perfStats[key] = 0;
        }

        // [中文注释] 从图集缓冲区获取颜色的辅助函数 (CPU 端)
        // 
        // 坐标系转换说明：
        // 1. OpenGL 纹理坐标 (uv) 原点在左下角。
        // 2. DOM/Canvas 坐标 (x,y) 原点在左上角。
        // 3. gl.readPixels 读取的数据是从下到上的 (Bottom-to-Top)。
        // 
        // 我们的图集布局 (在 Buffer 中)：
        // - 索引 0 (Buffer 头部) 对应 FBO 底部 (y=0)。
        // - 底部存的是亮度数据 (Luma)。
        // - 顶部存的是色彩数据 (Color)。
        // 
        // 所以 Color 数据的 Buffer 偏移量在后面。
        function getColorFromAtlas(x, y, buffer, atlasW, atlasH, origW, origH) {
            // 1. 将输入的 Y (Top-Left) 转换为 GL Y (Bottom-Left)
            let y_gl = origH - y;
            if (y_gl < 0) y_gl = 0; if (y_gl >= origH) y_gl = origH - 1;

            // 2. 映射到色彩图的坐标空间 (缩小了 0.25 倍)
            let x_c = Math.floor(x * colorScale);
            let y_c = Math.floor(y_gl * colorScale);

            // 3. 计算 Buffer 中的行号
            // 亮度数据占据了 Buffer 的前 origH 行 (Buffer 坐标)。
            // 色彩数据紧随其后。
            let colorH = Math.floor(origH * colorScale);
            if (y_c >= colorH) y_c = colorH - 1;

            let row = origH + y_c;

            // 4. 计算最终索引
            let idx = (row * atlasW + x_c) * 4;

            if (idx < 0 || idx >= buffer.length) return { r: 0, g: 0, b: 0 };

            return {
                r: buffer[idx],
                g: buffer[idx + 1],
                b: buffer[idx + 2]
            };
        }

        function renderScene() {
            let t0, t1, tTotal = performance.now();

            // 0. 更新图集 FBO 尺寸 (如果分辨率变了)
            initAtlasFBO(WIDTH, HEIGHT);

            // [Safety] Check if WebGL is initialized
            if (!programVideo || !textureVideo) {
                // Not ready yet
                return;
            }

            // [Safety] Check if OpenCV/Mats are initialized
            if (!matPool.src || !matPool.gray) {
                if (typeof cv !== 'undefined') {
                    initMats();
                }
                // If still not ready, we can process, but skip CV parts? 
                // Actually if matPool is empty, we must skip CV parts to avoid crash.
            }

            // ===============================================
            // 1. 绘制底图 (WebGL) - 可视化用
            // ===============================================
            gl.bindFramebuffer(gl.FRAMEBUFFER, null); // 绑定回屏幕
            gl.viewport(0, 0, WIDTH, HEIGHT);
            gl.useProgram(programVideo);
            gl.bindBuffer(gl.ARRAY_BUFFER, bufferVideoRect);

            let posLoc = gl.getAttribLocation(programVideo, "a_position");
            let texLoc = gl.getAttribLocation(programVideo, "a_texCoord");

            gl.enableVertexAttribArray(posLoc);
            gl.vertexAttribPointer(posLoc, 2, gl.FLOAT, false, 16, 0);
            gl.enableVertexAttribArray(texLoc);
            gl.vertexAttribPointer(texLoc, 2, gl.FLOAT, false, 16, 8);

            gl.activeTexture(gl.TEXTURE0);
            gl.bindTexture(gl.TEXTURE_2D, textureVideo);
            gl.pixelStorei(gl.UNPACK_ALIGNMENT, 1); // Ensure alignment

            if (isStaticMode) {
                if (textureNeedsUpdate) {
                    gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, uploadedImage);
                    let err = gl.getError();
                    if (err !== gl.NO_ERROR) {
                        logToUI("GL Error uploading texture: " + err, true);
                        console.error("GL Error uploading texture: " + err);
                        if (window.etsProxy) window.etsProxy.postMessage(JSON.stringify({ type: 'error', msg: 'GL Texture Error: ' + err }));
                    } else {
                        logToUI("Texture uploaded successfully");
                    }
                    textureNeedsUpdate = false;
                }
            } else {
                if (video.readyState >= 2) {
                    gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, video);
                }
            }
            gl.uniform1i(gl.getUniformLocation(programVideo, "u_image"), 0);
            gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);

            // ===============================================
            // 2. 绘制到图集 FBO (数据压缩处理)
            // ===============================================
            t0 = performance.now();
            gl.bindFramebuffer(gl.FRAMEBUFFER, atlasFbo);
            gl.viewport(0, 0, atlasWidth, atlasHeight);
            gl.disable(gl.SCISSOR_TEST);

            // Pass 1: 打包亮度 (绘制到 FBO 底部区域)
            // 视口高度设为 HEIGHT，对应 FBO 纹理的 0..HEIGHT 行
            gl.viewport(0, 0, atlasWidth, HEIGHT);
            gl.useProgram(programPackLuma);
            let attPosP = gl.getAttribLocation(programPackLuma, "a_position");
            let attTexP = gl.getAttribLocation(programPackLuma, "a_texCoord");
            gl.enableVertexAttribArray(attPosP);
            gl.vertexAttribPointer(attPosP, 2, gl.FLOAT, false, 16, 0);
            gl.enableVertexAttribArray(attTexP);
            gl.vertexAttribPointer(attTexP, 2, gl.FLOAT, false, 16, 8);

            gl.uniform1i(gl.getUniformLocation(programPackLuma, "u_image"), 0);
            gl.uniform1f(gl.getUniformLocation(programPackLuma, "u_width"), WIDTH);
            gl.uniform1f(gl.getUniformLocation(programPackLuma, "u_height"), HEIGHT);
            gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);

            // Pass 2: 降采样色彩 (绘制到 FBO 顶部区域)
            // y 偏移量为 HEIGHT，即接着亮度数据往上画
            let colorH = Math.ceil(HEIGHT * colorScale);
            gl.viewport(0, HEIGHT, atlasWidth, colorH);
            gl.useProgram(programDownscaleColor);

            let attPosC = gl.getAttribLocation(programDownscaleColor, "a_position");
            let attTexC = gl.getAttribLocation(programDownscaleColor, "a_texCoord");
            gl.enableVertexAttribArray(attPosC);
            gl.vertexAttribPointer(attPosC, 2, gl.FLOAT, false, 16, 0);
            gl.enableVertexAttribArray(attTexC);
            gl.vertexAttribPointer(attTexC, 2, gl.FLOAT, false, 16, 8);

            gl.uniform1i(gl.getUniformLocation(programDownscaleColor, "u_image"), 0);
            gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);

            t1 = performance.now();

            // ===============================================
            // 3. 读取像素 (readPixels)
            // ===============================================
            // [性能核心] 此时读取的数据量仅为原来的 ~30% (取决于 colorScale)
            t0 = performance.now();
            let totalBytes = atlasWidth * atlasHeight * 4;
            if (!matPool.pixelBuff || matPool.pixelBuff.length !== totalBytes) {
                matPool.pixelBuff = new Uint8Array(totalBytes);
            }
            gl.readPixels(0, 0, atlasWidth, atlasHeight, gl.RGBA, gl.UNSIGNED_BYTE, matPool.pixelBuff);

            t1 = performance.now();
            perfStats.getImageData += (t1 - t0);

            // ===============================================
            // 4. 解码数据 / OpenCV 准备
            // ===============================================
            let p = matPool;

            // [Bright Decode] 快速拷贝亮度数据
            // WebGL 读取的数据是 Bottom-to-Top 的，所以我们得到的 p.gray 实际上是倒立的图。
            // 这没关系，我们后续处理统一当做倒立图来做，最后画星星时翻转坐标即可。
            t0 = performance.now();
            let lumaByteLength = WIDTH * HEIGHT;

            // 检查步长 (Stride) 是否对齐。
            // atlasWidth * 4 是缓冲区的一行字节数。
            // WIDTH 是我们需要的一行字节数。
            // 只有当分辨率宽度是 4 的倍数时，这两者才相等，可以做极速的整块拷贝。
            if ((atlasWidth * 4) === WIDTH) {
                p.gray.data.set(matPool.pixelBuff.subarray(0, lumaByteLength));
            } else {
                // 如果不对齐 (比如宽度不是 4 的倍数)，需要逐行拷贝
                let rowBytes = WIDTH;
                let strideBytes = atlasWidth * 4;
                for (let r = 0; r < HEIGHT; r++) {
                    let srcStart = r * strideBytes;
                    let dstStart = r * rowBytes;
                    p.gray.data.set(matPool.pixelBuff.subarray(srcStart, srcStart + rowBytes), dstStart);
                }
            }

            perfStats.cvtColor += (performance.now() - t0);

            // ===============================================
            // 5. OpenCV 核心处理 (检测星芒候选点)
            // ===============================================

            // [Safety] If mats are not ready, skip star detection
            if (!matPool.gray || !matPool.src) {
                // Just quit here, we already drew the background
                perfStats.total += (performance.now() - tTotal);
                perfStats.sampleCount++;
                return;
            }

            let detectionMat = p.gray; // 注意：此时图像是倒立的！
            let scale = params.detectionScale;

            // [缩放] 为了性能，可以在更小的图上做检测
            if (scale < 1.0) {
                t0 = performance.now();
                cv.resize(p.gray, p.smallGray, new cv.Size(0, 0), scale, scale, cv.INTER_LINEAR);
                detectionMat = p.smallGray;
                t1 = performance.now();
                perfStats.resize += (t1 - t0);
            } else {
                perfStats.resize += 0;
            }

            // [阈值] 提取高光点
            t0 = performance.now();
            let threshMat = (scale < 1.0) ? p.smallThresh : p.thresh;
            cv.threshold(detectionMat, threshMat, params.threshold, 255, cv.THRESH_BINARY);
            t1 = performance.now();
            perfStats.threshold += (t1 - t0);

            // [膨胀] 连接断开的亮斑
            t0 = performance.now();
            if (params.dilationEnabled) {
                let M = cv.Mat.ones(params.dilationSize, params.dilationSize, cv.CV_8U);
                let anchor = new cv.Point(-1, -1);
                cv.dilate(threshMat, threshMat, M, anchor, params.dilationIter, cv.BORDER_CONSTANT, cv.morphologyDefaultBorderValue());
                M.delete();
            }
            t1 = performance.now();
            perfStats.dilate += (t1 - t0);

            // [查找轮廓]
            t0 = performance.now();
            cv.findContours(threshMat, p.contours, p.hierarchy, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE);
            t1 = performance.now();
            perfStats.findContours += (t1 - t0);

            // [遍历轮廓] 筛选星星候选
            t0 = performance.now();
            let candidates = [];
            let maxAreaVal = (WIDTH * HEIGHT) / params.maxAreaRatio;
            let invScale = 1.0 / scale;

            for (let i = 0; i < p.contours.size(); ++i) {
                let contour = p.contours.get(i);
                let areaSmall = cv.contourArea(contour);

                let area = areaSmall * (invScale * invScale);

                if (params.minAreaEnabled && area < params.minArea) continue;
                if (params.maxAreaEnabled && area > maxAreaVal) continue;

                let rectSmall = cv.boundingRect(contour);
                let rect = {
                    x: Math.round(rectSmall.x * invScale),
                    y: Math.round(rectSmall.y * invScale),
                    width: Math.round(rectSmall.width * invScale),
                    height: Math.round(rectSmall.height * invScale)
                };

                if (rect.x < 0) rect.x = 0;
                if (rect.y < 0) rect.y = 0;
                if (rect.x + rect.width > WIDTH) rect.width = WIDTH - rect.x;
                if (rect.y + rect.height > HEIGHT) rect.height = HEIGHT - rect.y;

                if (params.shapeFilterEnabled) {
                    let perimeter = cv.arcLength(contour, true) * invScale;
                    if (perimeter === 0) continue;
                    let circularity = 4 * Math.PI * area / (perimeter * perimeter);
                    if (circularity < params.circularity) continue;
                }

                // [亚像素级光源追踪] 
                // 计算加权质心 (Weighted Centroid) 以获得极其稳定的星星中心。
                // 注意：因为图像是倒立的，这里计算出的 cy 也是倒立空间下的坐标。
                let cxSub = 0, cySub = 0, totalWeight = 0;
                let grayData = p.gray.data;
                let threshValue = params.threshold;

                for (let py = rect.y; py < rect.y + rect.height; py++) {
                    for (let px = rect.x; px < rect.x + rect.width; px++) {
                        let idx = py * WIDTH + px;
                        let brightness = grayData[idx];
                        if (brightness > threshValue) {
                            let weight = brightness * brightness;
                            cxSub += px * weight;
                            cySub += py * weight;
                            totalWeight += weight;
                        }
                    }
                }

                let cx, cy;
                if (totalWeight > 0) {
                    cx = cxSub / totalWeight;
                    cy = cySub / totalWeight;
                } else {
                    cx = rect.x + rect.width / 2;
                    cy = rect.y + rect.height / 2;
                }

                candidates.push({ area: area, cx: cx, cy: cy });
            }
            t1 = performance.now();
            perfStats.contourLoop += (t1 - t0);

            candidates.sort((a, b) => b.area - a.area);
            if (candidates.length > 40) candidates.length = 40;

            // ===============================================
            // 6. 绘制星芒 (WebGL)
            // ===============================================
            t0 = performance.now();
            gl.bindFramebuffer(gl.FRAMEBUFFER, null);
            gl.viewport(0, 0, WIDTH, HEIGHT);
            gl.useProgram(programStar);
            gl.enable(gl.BLEND);
            gl.blendFunc(gl.ONE, gl.ONE);

            gl.uniform2f(gl.getUniformLocation(programStar, "u_resolution"), WIDTH, HEIGHT);
            let locColor = gl.getUniformLocation(programStar, "u_color");
            let locOpacity = gl.getUniformLocation(programStar, "u_opacity");
            let locPos = gl.getAttribLocation(programStar, "a_position");
            let locGrad = gl.getAttribLocation(programStar, "a_gradient");

            gl.bindBuffer(gl.ARRAY_BUFFER, bufferStar);

            for (let i = 0; i < candidates.length; i++) {
                let cand = candidates[i];

                // [色彩采样] 
                // 这里的 cand.cy 是在倒立图中的 Y 坐标 (0 在底部)。
                // 我们的 Buffer 布局是：底部是 Luma，顶部是 Color。
                // 我们需要算出该点相对于图像顶部的距离，去 Buffer 的色彩区域采样。
                // y_top = HEIGHT - cand.cy
                let y_top = HEIGHT - cand.cy;
                let rawColor = getColorFromAtlas(cand.cx, y_top, matPool.pixelBuff, atlasWidth, atlasHeight, WIDTH, HEIGHT);
                let finalColor = adjustSaturation(rawColor.r, rawColor.g, rawColor.b, params.starSaturation);

                // [坐标翻转]
                // WebGL 绘制时，如果直接用 cand.cy (倒立坐标)，星星会画反。
                // 需要翻转回屏幕坐标：cy_disp = HEIGHT - cand.cy
                let cx_disp = cand.cx;
                let cy_disp = HEIGHT - cand.cy;

                // 动态调整星芒大小
                let currentLen = params.starLengthMax;
                let currentWidthScale = 1.0;
                let currentOpacity = params.starOpacity;

                if (params.starLengthMin < params.starLengthMax) {
                    let areaRatio = (cand.area - params.minArea) / (maxAreaVal - params.minArea);
                    if (areaRatio < 0) areaRatio = 0; if (areaRatio > 1) areaRatio = 1;
                    currentLen = params.starLengthMin + (params.starLengthMax - params.starLengthMin) * Math.sqrt(areaRatio);
                    currentWidthScale = 1.0 + areaRatio * 2.0;
                    currentOpacity = params.starOpacity * (0.6 + 0.4 * Math.sqrt(areaRatio));
                }

                let vertices = [];
                addStarVertices(vertices, cx_disp, cy_disp, currentLen * 2, params.starPoints, params.starAngle, currentWidthScale);

                gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(vertices), gl.DYNAMIC_DRAW);

                gl.uniform4f(locColor, finalColor.r / 255, finalColor.g / 255, finalColor.b / 255, 1.0);
                gl.uniform1f(locOpacity, currentOpacity);

                gl.enableVertexAttribArray(locPos);
                gl.vertexAttribPointer(locPos, 2, gl.FLOAT, false, 12, 0);
                gl.enableVertexAttribArray(locGrad);
                gl.vertexAttribPointer(locGrad, 1, gl.FLOAT, false, 12, 8);

                gl.drawArrays(gl.TRIANGLES, 0, vertices.length / 3);
            }

            gl.disable(gl.BLEND);
            t1 = performance.now();
            perfStats.webglDraw += (t1 - t0);

            perfStats.total += (performance.now() - tTotal);
            perfStats.sampleCount++;
        }

        function processVideo() {
            if (!streaming) return;
            try {
                renderScene();

                frameCount++;
                let now = Date.now();
                if (now - lastTime >= 1000) {
                    if (window.etsProxy) window.etsProxy.postMessage(JSON.stringify({ type: 'fps', value: frameCount }));
                    // 输出性能统计
                    logPerformance();
                    frameCount = 0;
                    lastTime = now;
                }
                animationId = requestAnimationFrame(processVideo);
            } catch (err) {
                console.error(err);
                animationId = requestAnimationFrame(processVideo);
            }
        }
    </script>
</body>

</html>